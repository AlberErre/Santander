{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies for creating a data frame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import csv\n",
    "\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create RDD from csv files\n",
    "trainRDD = spark.read.csv(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\", \n",
    "                          header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "testRDD = spark.read.csv(\"hdfs://saltdean/data/data/santander-products/test_ver2.csv\", \n",
    "                          header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively...\n",
    "# create RDD from csv files\n",
    "trainRDD = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "trainRDD = trainRDD.mapPartitions(lambda x: csv.reader(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively... from https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "# create RDD from csv files\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "elements = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "clients = elements.map(lambda p: (p[0], p[1].strip(),p[2],...))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age ...\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD and register the DataFrame to be used with Spark SQL.\n",
    "trainRDD = spark.createDataFrame(clients, schema)\n",
    "trainRDD.createOrReplaceTempView('trainingset')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively, as seen in tutorial 8:\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "trainRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# Create DataFrame and register it to be used with Spark SQL.\n",
    "trainClients = spark.createDataFrame(trainRDD)\n",
    "trainClients.createOrReplaceTempView('Clients')\n",
    "\n",
    "# For testing\n",
    "print(trainClients.describe()) # columns info\n",
    "print(trainClients.count()) # number of instances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies for machine learning pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (donâ€™t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fine-tuning (hyperparameters optimization). (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
