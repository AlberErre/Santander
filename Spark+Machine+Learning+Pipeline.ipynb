{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Summary of machine learning pipeline\n",
    "Step 1.  \n",
    "Step 2.  \n",
    "Step 3.  \n",
    "Step 4.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading data to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies for creating a data frame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import csv\n",
    "\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create RDD from csv files\n",
    "trainRDD = spark.read.csv(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\", \n",
    "                          header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively...\n",
    "# create RDD from csv files\n",
    "trainRDD = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "trainRDD = trainRDD.mapPartitions(lambda x: csv.reader(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively... from https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "# create RDD from csv files\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "elements = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "clients = elements.map(lambda p: (p[0], p[1].strip(),p[2],...))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age ...\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD and register the DataFrame to be used with Spark SQL.\n",
    "trainRDD = spark.createDataFrame(clients, schema)\n",
    "trainRDD.createOrReplaceTempView('trainingset')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively, as seen in tutorial 8:\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "trainRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# Create DataFrame and register it to be used with Spark SQL.\n",
    "trainData = spark.createDataFrame(trainRDD)\n",
    "trainData.createOrReplaceTempView('Clients')\n",
    "\n",
    "# For testing\n",
    "print(trainData.describe()) # columns info\n",
    "print(trainData.count()) # number of instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code modified from \n",
    "# https://www.kaggle.com/apryor6/santander-product-recommendation/detailed-cleaning-visualization-python/notebook\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "\n",
    "# One-Hot Encoding\n",
    "categoricalColumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"j\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\") # Category Indexing with StringIndexer\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    stages += [stringIndexer, encoder]  # Add stages to the pipeline\n",
    "    \n",
    "# Convert labels into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"add here target column in csv file\", outputCol = \"labels\")\n",
    "stages += [label_stringIdx]  # Add stage to the pipeline\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"m\", \"n\", \"o\", \"p\", \"q\", \"r\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"labels\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=100,                 #  Number of trees in the random forest\n",
    "                            impurity='entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy=\"auto\",\n",
    "                            predictionCol=\"prediction\"\n",
    "                            maxDepth=5, \n",
    "                            maxBins=32, \n",
    "                            minInstancesPerNode=1, \n",
    "                            minInfoGain=0.0, \n",
    "                            subsamplingRate=1.0)\n",
    "stages += [rf]  # Add stage to the pipeline\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (donâ€™t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split data into training set and testing set\n",
    "[trainData, testData] = trainData.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "# Train model in pipeline\n",
    "rfModel = pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "predictions = rfModel.transform(trainData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(train_pipeline.stages[0])  # summary\n",
    "\n",
    "\n",
    "# Run the feature transformations pipeline on the test data set\n",
    "pipelineModel = prePro_pipeline.fit(testClients)  #  computes feature statistics\n",
    "testData = pipelineModel.transform(testClients)  #  transforms the features\n",
    "\n",
    "# Make predictions for test set and compute test error\n",
    "test_predictions = rfModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fine-tuning. (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n=== training set of size 100%')\n",
    "# Train model in pipeline\n",
    "tempModel = pipeline.fit(trainData)\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "tempPredictions = tempModel.transform(trainData)\n",
    "tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "\n",
    "for x in data:\n",
    "    print('\\n=== training set of size reduced to %g' % x)\n",
    "    # Train model in pipeline\n",
    "    tempModel = pipeline.fit(data[x])\n",
    "    # Make predictions for training set and compute training set accuracy\n",
    "    tempPredictions = tempModel.transform(data[x])\n",
    "    tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,20,50,100,200,500,1000,5000]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [0,1,2,4,6,8,10]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,10,20,50]).build()\n",
    "\n",
    "# Grid Search and Cross Validation\n",
    "crossVal = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "rfCrosVal = crossVal.fit(trainData)\n",
    "print('Grid Search has finished')\n",
    "\n",
    "print(rfCrosVal.bestModel.rank)\n",
    "paramMap = list(zip(rfCrosVal.getEstimatorParamMaps(),rfCrosVal.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the model with test data\n",
    "cvtest_predictions = rfCrosVal.transform(testData)\n",
    "cvtest_accuracy = evaluator.evaluate(cvtest_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - cvtest_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
