{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Summary of machine learning pipeline\n",
    "Step 1.  \n",
    "Step 2.  \n",
    "Step 3.  \n",
    "Step 4.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading data to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies for creating a data frame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import csv\n",
    "\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create RDD from csv files\n",
    "trainRDD = spark.read.csv(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\", \n",
    "                          header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "testRDD = spark.read.csv(\"hdfs://saltdean/data/data/santander-products/test_ver2.csv\", \n",
    "                          header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively...\n",
    "# create RDD from csv files\n",
    "trainRDD = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "trainRDD = trainRDD.mapPartitions(lambda x: csv.reader(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively... from https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "# create RDD from csv files\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "elements = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "clients = elements.map(lambda p: (p[0], p[1].strip(),p[2],...))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age ...\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD and register the DataFrame to be used with Spark SQL.\n",
    "trainRDD = spark.createDataFrame(clients, schema)\n",
    "trainRDD.createOrReplaceTempView('trainingset')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alternatively, as seen in tutorial 8:\n",
    "lines = sc.textFile(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "trainRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# Create DataFrame and register it to be used with Spark SQL.\n",
    "trainClients = spark.createDataFrame(trainRDD)\n",
    "trainClients.createOrReplaceTempView('Clients')\n",
    "\n",
    "# For testing\n",
    "print(trainClients.describe()) # columns info\n",
    "print(trainClients.count()) # number of instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate average permanency of clients in training set\n",
    "SQL1 = 'SELECT AVG(antiguedad) FROM trainClients'\n",
    "row = spark.sql(SQL1).collect()[0] # get the single row with the result\n",
    "\n",
    "meanTime = row['avg(antiguedad)'] # access Row as a map \n",
    "print('meanTime(months)',meanTime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# One-Hot Encoding\n",
    "categoricalColumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"j\"]\n",
    "stages = [] # stages in the Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\") # Category Indexing with StringIndexer\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    stages += [stringIndexer, encoder]  # Add stages to the pipeline\n",
    "\n",
    "    \n",
    "# Convert labels into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"add here target column in csv file\", outputCol = \"labels\")\n",
    "stages += [label_stringIdx]  # Add stage to the pipeline\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"m\", \"n\", \"o\", \"p\", \"q\", \"r\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "# Create data pre-processing Pipeline.\n",
    "prePro_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Run the feature transformations pipeline on the training data set\n",
    "pipelineModel = prePro_pipeline.fit(trainClients)         #  computes feature statistics\n",
    "trainData = pipelineModel.transform(trainClients)  #  transforms the features\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"labels\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=100,                 #  Number of trees in the random forest\n",
    "                            impurity='entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy=\"auto\",\n",
    "                            maxDepth=5, \n",
    "                            maxBins=32, \n",
    "                            minInstancesPerNode=1, \n",
    "                            minInfoGain=0.0, \n",
    "                            subsamplingRate=1.0)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=label_stringIdx.labels)\n",
    "\n",
    "# Model Training Pipeline\n",
    "train_pipeline = Pipeline(stages=[rf, labelConverter])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (donâ€™t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Train model in pipeline\n",
    "rfModel = train_pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "predictions = rfModel.transform(trainData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(train_pipeline.stages[0])  # summary\n",
    "\n",
    "\n",
    "# Run the feature transformations pipeline on the test data set\n",
    "pipelineModel = prePro_pipeline.fit(testClients)  #  computes feature statistics\n",
    "testData = pipelineModel.transform(testClients)  #  transforms the features\n",
    "\n",
    "# Make predictions for test set and compute test error\n",
    "test_predictions = rfModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fine-tuning (hyperparameters optimization). (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,20,50,100,200,500,1000,5000]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [0,1,2,4,6,8,10]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,10,20,50]).build()\n",
    "\n",
    "# Grid Search and Cross Validation\n",
    "crossVal = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "print('starting cross-validation')\n",
    "rfCrosVal = crossVal.fit(trainData)\n",
    "print('finished cross-validation')\n",
    "\n",
    "print(rfCrosVal.bestModel.rank)\n",
    "paramMap = list(zip(rfCrosVal.getEstimatorParamMaps(),rfCrosVal.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the model with test data\n",
    "cvtest_predictions = rfCrosVal.transform(testData)\n",
    "cvtest_accuracy = evaluator.evaluate(cvtest_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - cvtest_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
