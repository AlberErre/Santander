{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline - Task 2 (Big Data Module)\n",
    "\n",
    "Students:  \n",
    "Miguel Esteras & Alberto Ruiz Benitez de Lugo\n",
    "\n",
    "This coursework contains an implementation and application of Spark Machine Learning Pipelines. The piepeline is evaluated regarding preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section A) Choice of dataset and task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Santander products <-- DataSet\n",
    "\n",
    "The dataset used is called \"Santander Products\", part of the Kaggle competition with the same name. The reason of this choice has been the large amount of predictors and responses available within this data. This amount of possible variable combinations increases the complexity of the problem and therefore the selected predictor model should have a deep level of abstraction. Furthermore, this example will appropriately showcase the advantages of using Spark parallel computing for the analysis of large datasets. \n",
    "\n",
    "The chosen model, \"Random Forest\" (RF), is able to accommodate large numbers of features of diverse type. This method is currently state-of-the-art in many different Machine Learning fields, like computer vision. RF is expected to reach a good performance in both, classification and regression implementations. \n",
    "\n",
    "### Task\n",
    "\n",
    "The goal of the pipeline is to predict whether a financial product (a mortgage) will be purchased by a consumer, given the personal and financial data available for each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section B) Machine Learning Pipeline in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Summary of Pipeline\n",
    "\n",
    "- Load Data and first preprocessing (1.2)\n",
    "- Descriptive statistics (1.3)\n",
    "- Data Cleaning (1.4)\n",
    "- Machine learning pipeline Implementation using Random Forest (2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type_dict = {'ncodpers':np.int32,\n",
    "            'ind_ahor_fin_ult1':np.uint8, 'ind_aval_fin_ult1':np.uint8, \n",
    "            'ind_cco_fin_ult1':np.uint8,'ind_cder_fin_ult1':np.uint8,\n",
    "            'ind_cno_fin_ult1':np.uint8,'ind_ctju_fin_ult1':np.uint8,'ind_ctma_fin_ult1':np.uint8,\n",
    "            'ind_ctop_fin_ult1':np.uint8,'ind_ctpp_fin_ult1':np.uint8,'ind_deco_fin_ult1':np.uint8,\n",
    "            'ind_deme_fin_ult1':np.uint8,'ind_dela_fin_ult1':np.uint8,'ind_ecue_fin_ult1':np.uint8,\n",
    "            'ind_fond_fin_ult1':np.uint8,'ind_hip_fin_ult1':np.uint8,'ind_plan_fin_ult1':np.uint8,\n",
    "            'ind_pres_fin_ult1':np.uint8,'ind_reca_fin_ult1':np.uint8,'ind_tjcr_fin_ult1':np.uint8,\n",
    "            'ind_valo_fin_ult1':np.uint8,'ind_viv_fin_ult1':np.uint8, 'ind_recibo_ult1':np.uint8 }\n",
    "\n",
    "# load data from server into dataframe (only loading the top 10,000,000 for demonstration purpose)\n",
    "df = pd.read_csv(\"/data/tempstore/santander-products/train_ver2.csv\",\n",
    "                 nrows = 10000000,\n",
    "                 dtype = type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>indrel</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>cod_prov</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>renta</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>ind_cco_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>9.972266e+06</td>\n",
       "      <td>9.972266e+06</td>\n",
       "      <td>9972265.0</td>\n",
       "      <td>9.922359e+06</td>\n",
       "      <td>9.972266e+06</td>\n",
       "      <td>8.087228e+06</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>9.983937e+06</td>\n",
       "      <td>9.983937e+06</td>\n",
       "      <td>1.000000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.225624e+05</td>\n",
       "      <td>5.788785e-02</td>\n",
       "      <td>1.177470e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.658602e+01</td>\n",
       "      <td>4.693726e-01</td>\n",
       "      <td>1.341962e+05</td>\n",
       "      <td>1.085000e-04</td>\n",
       "      <td>2.520000e-05</td>\n",
       "      <td>6.738647e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.211100e-03</td>\n",
       "      <td>9.572500e-03</td>\n",
       "      <td>2.788400e-03</td>\n",
       "      <td>5.379230e-02</td>\n",
       "      <td>4.677560e-02</td>\n",
       "      <td>2.637820e-02</td>\n",
       "      <td>4.061500e-03</td>\n",
       "      <td>5.614519e-02</td>\n",
       "      <td>6.083672e-02</td>\n",
       "      <td>1.305716e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.253517e+05</td>\n",
       "      <td>2.335313e-01</td>\n",
       "      <td>4.166606e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.276671e+01</td>\n",
       "      <td>4.990611e-01</td>\n",
       "      <td>2.298643e+05</td>\n",
       "      <td>1.041577e-02</td>\n",
       "      <td>5.019897e-03</td>\n",
       "      <td>4.687975e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.856540e-02</td>\n",
       "      <td>9.736975e-02</td>\n",
       "      <td>5.273163e-02</td>\n",
       "      <td>2.256074e-01</td>\n",
       "      <td>2.111579e-01</td>\n",
       "      <td>1.602573e-01</td>\n",
       "      <td>6.360035e-02</td>\n",
       "      <td>2.302019e-01</td>\n",
       "      <td>2.390306e-01</td>\n",
       "      <td>3.369313e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.588900e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.202730e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.453060e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.872760e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.246530e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.018658e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.186443e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.559505e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.528598e+06</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.889440e+07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ncodpers     ind_nuevo        indrel    tipodom      cod_prov  \\\n",
       "count  1.000000e+07  9.972266e+06  9.972266e+06  9972265.0  9.922359e+06   \n",
       "mean   8.225624e+05  5.788785e-02  1.177470e+00        1.0  2.658602e+01   \n",
       "std    4.253517e+05  2.335313e-01  4.166606e+00        0.0  1.276671e+01   \n",
       "min    1.588900e+04  0.000000e+00  1.000000e+00        1.0  1.000000e+00   \n",
       "25%    4.453060e+05  0.000000e+00  1.000000e+00        1.0  1.500000e+01   \n",
       "50%    9.246530e+05  0.000000e+00  1.000000e+00        1.0  2.800000e+01   \n",
       "75%    1.186443e+06  0.000000e+00  1.000000e+00        1.0  3.500000e+01   \n",
       "max    1.528598e+06  1.000000e+00  9.900000e+01        1.0  5.200000e+01   \n",
       "\n",
       "       ind_actividad_cliente         renta  ind_ahor_fin_ult1  \\\n",
       "count           9.972266e+06  8.087228e+06       1.000000e+07   \n",
       "mean            4.693726e-01  1.341962e+05       1.085000e-04   \n",
       "std             4.990611e-01  2.298643e+05       1.041577e-02   \n",
       "min             0.000000e+00  1.202730e+03       0.000000e+00   \n",
       "25%             0.000000e+00  6.872760e+04       0.000000e+00   \n",
       "50%             0.000000e+00  1.018658e+05       0.000000e+00   \n",
       "75%             1.000000e+00  1.559505e+05       0.000000e+00   \n",
       "max             1.000000e+00  2.889440e+07       1.000000e+00   \n",
       "\n",
       "       ind_aval_fin_ult1  ind_cco_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "count       1.000000e+07      1.000000e+07       ...             1.000000e+07   \n",
       "mean        2.520000e-05      6.738647e-01       ...             6.211100e-03   \n",
       "std         5.019897e-03      4.687975e-01       ...             7.856540e-02   \n",
       "min         0.000000e+00      0.000000e+00       ...             0.000000e+00   \n",
       "25%         0.000000e+00      0.000000e+00       ...             0.000000e+00   \n",
       "50%         0.000000e+00      1.000000e+00       ...             0.000000e+00   \n",
       "75%         0.000000e+00      1.000000e+00       ...             0.000000e+00   \n",
       "max         1.000000e+00      1.000000e+00       ...             1.000000e+00   \n",
       "\n",
       "       ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "count       1.000000e+07       1.000000e+07       1.000000e+07   \n",
       "mean        9.572500e-03       2.788400e-03       5.379230e-02   \n",
       "std         9.736975e-02       5.273163e-02       2.256074e-01   \n",
       "min         0.000000e+00       0.000000e+00       0.000000e+00   \n",
       "25%         0.000000e+00       0.000000e+00       0.000000e+00   \n",
       "50%         0.000000e+00       0.000000e+00       0.000000e+00   \n",
       "75%         0.000000e+00       0.000000e+00       0.000000e+00   \n",
       "max         1.000000e+00       1.000000e+00       1.000000e+00   \n",
       "\n",
       "       ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "count       1.000000e+07       1.000000e+07      1.000000e+07   \n",
       "mean        4.677560e-02       2.637820e-02      4.061500e-03   \n",
       "std         2.111579e-01       1.602573e-01      6.360035e-02   \n",
       "min         0.000000e+00       0.000000e+00      0.000000e+00   \n",
       "25%         0.000000e+00       0.000000e+00      0.000000e+00   \n",
       "50%         0.000000e+00       0.000000e+00      0.000000e+00   \n",
       "75%         0.000000e+00       0.000000e+00      0.000000e+00   \n",
       "max         1.000000e+00       1.000000e+00      1.000000e+00   \n",
       "\n",
       "       ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "count     9.983937e+06       9.983937e+06     1.000000e+07  \n",
       "mean      5.614519e-02       6.083672e-02     1.305716e-01  \n",
       "std       2.302019e-01       2.390306e-01     3.369313e-01  \n",
       "min       0.000000e+00       0.000000e+00     0.000000e+00  \n",
       "25%       0.000000e+00       0.000000e+00     0.000000e+00  \n",
       "50%       0.000000e+00       0.000000e+00     0.000000e+00  \n",
       "75%       0.000000e+00       0.000000e+00     0.000000e+00  \n",
       "max       1.000000e+00       1.000000e+00     1.000000e+00  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               10000000\n",
       "ncodpers                 10000000\n",
       "ind_empleado              9972266\n",
       "pais_residencia           9972266\n",
       "sexo                      9972216\n",
       "age                      10000000\n",
       "fecha_alta                9972266\n",
       "ind_nuevo                 9972266\n",
       "antiguedad               10000000\n",
       "indrel                    9972266\n",
       "ult_fec_cli_1t              18059\n",
       "indrel_1mes               9868409\n",
       "tiprel_1mes               9868409\n",
       "indresi                   9972266\n",
       "indext                    9972266\n",
       "conyuemp                     1342\n",
       "canal_entrada             9843347\n",
       "indfall                   9972266\n",
       "tipodom                   9972265\n",
       "cod_prov                  9922359\n",
       "nomprov                   9922359\n",
       "ind_actividad_cliente     9972266\n",
       "renta                     8087228\n",
       "segmento                  9841339\n",
       "ind_ahor_fin_ult1        10000000\n",
       "ind_aval_fin_ult1        10000000\n",
       "ind_cco_fin_ult1         10000000\n",
       "ind_cder_fin_ult1        10000000\n",
       "ind_cno_fin_ult1         10000000\n",
       "ind_ctju_fin_ult1        10000000\n",
       "ind_ctma_fin_ult1        10000000\n",
       "ind_ctop_fin_ult1        10000000\n",
       "ind_ctpp_fin_ult1        10000000\n",
       "ind_deco_fin_ult1        10000000\n",
       "ind_deme_fin_ult1        10000000\n",
       "ind_dela_fin_ult1        10000000\n",
       "ind_ecue_fin_ult1        10000000\n",
       "ind_fond_fin_ult1        10000000\n",
       "ind_hip_fin_ult1         10000000\n",
       "ind_plan_fin_ult1        10000000\n",
       "ind_pres_fin_ult1        10000000\n",
       "ind_reca_fin_ult1        10000000\n",
       "ind_tjcr_fin_ult1        10000000\n",
       "ind_valo_fin_ult1        10000000\n",
       "ind_viv_fin_ult1         10000000\n",
       "ind_nomina_ult1           9983937\n",
       "ind_nom_pens_ult1         9983937\n",
       "ind_recibo_ult1          10000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only unique id\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "df = df[df.ncodpers.isin(unique_ids)]  \n",
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# eliminate mostly empty columns and redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\", \"ult_fec_cli_1t\",\"conyuemp\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform to numeric and set missing values to nan\n",
    "df['age']=pd.to_numeric(df.age, errors='coerce')\n",
    "df['ind_nuevo']=pd.to_numeric(df.ind_nuevo, errors='coerce')\n",
    "df['antiguedad']=pd.to_numeric(df.antiguedad, errors='coerce')\n",
    "df['indrel']=pd.to_numeric(df.indrel, errors='coerce')\n",
    "df['renta']=pd.to_numeric(df.renta, errors='coerce')\n",
    "df['indrel_1mes']=pd.to_numeric(df.indrel_1mes, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from age variable\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-28T00:00:00.000000000', '2015-02-28T00:00:00.000000000',\n",
       "       '2015-03-28T00:00:00.000000000', '2015-04-28T00:00:00.000000000',\n",
       "       '2015-05-28T00:00:00.000000000', '2015-06-28T00:00:00.000000000',\n",
       "       '2015-07-28T00:00:00.000000000', '2015-08-28T00:00:00.000000000',\n",
       "       '2015-09-28T00:00:00.000000000', '2015-10-28T00:00:00.000000000',\n",
       "       '2015-11-28T00:00:00.000000000', '2015-12-28T00:00:00.000000000',\n",
       "       '2016-01-28T00:00:00.000000000', '2016-02-28T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfor dates to datetime datatype\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fill datetime missing values\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado              True\n",
       "pais_residencia           True\n",
       "sexo                      True\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                 True\n",
       "antiguedad                True\n",
       "indrel                    True\n",
       "indrel_1mes               True\n",
       "tiprel_1mes               True\n",
       "indresi                   True\n",
       "indext                    True\n",
       "canal_entrada             True\n",
       "indfall                   True\n",
       "nomprov                   True\n",
       "ind_actividad_cliente     True\n",
       "renta                     True\n",
       "segmento                  True\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1           True\n",
       "ind_nom_pens_ult1         True\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace other missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove rows with any nan value left\n",
    "df = df.dropna(subset=['renta', 'segmento', 'canal_entrada', 'ind_empleado', \n",
    "                       'pais_residencia', 'indresi', 'indresi', 'sexo'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado             False\n",
       "pais_residencia          False\n",
       "sexo                     False\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                False\n",
       "antiguedad               False\n",
       "indrel                   False\n",
       "indrel_1mes              False\n",
       "tiprel_1mes              False\n",
       "indresi                  False\n",
       "indext                   False\n",
       "canal_entrada            False\n",
       "indfall                  False\n",
       "nomprov                  False\n",
       "ind_actividad_cliente    False\n",
       "renta                    False\n",
       "segmento                 False\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1          False\n",
       "ind_nom_pens_ult1        False\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values are gone\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               8037851\n",
       "ncodpers                 8037851\n",
       "ind_empleado             8037851\n",
       "pais_residencia          8037851\n",
       "sexo                     8037851\n",
       "age                      8037851\n",
       "fecha_alta               8037851\n",
       "ind_nuevo                8037851\n",
       "antiguedad               8037851\n",
       "indrel                   8037851\n",
       "indrel_1mes              8037851\n",
       "tiprel_1mes              8037851\n",
       "indresi                  8037851\n",
       "indext                   8037851\n",
       "canal_entrada            8037851\n",
       "indfall                  8037851\n",
       "nomprov                  8037851\n",
       "ind_actividad_cliente    8037851\n",
       "renta                    8037851\n",
       "segmento                 8037851\n",
       "ind_ahor_fin_ult1        8037851\n",
       "ind_aval_fin_ult1        8037851\n",
       "ind_cco_fin_ult1         8037851\n",
       "ind_cder_fin_ult1        8037851\n",
       "ind_cno_fin_ult1         8037851\n",
       "ind_ctju_fin_ult1        8037851\n",
       "ind_ctma_fin_ult1        8037851\n",
       "ind_ctop_fin_ult1        8037851\n",
       "ind_ctpp_fin_ult1        8037851\n",
       "ind_deco_fin_ult1        8037851\n",
       "ind_deme_fin_ult1        8037851\n",
       "ind_dela_fin_ult1        8037851\n",
       "ind_ecue_fin_ult1        8037851\n",
       "ind_fond_fin_ult1        8037851\n",
       "ind_hip_fin_ult1         8037851\n",
       "ind_plan_fin_ult1        8037851\n",
       "ind_pres_fin_ult1        8037851\n",
       "ind_reca_fin_ult1        8037851\n",
       "ind_tjcr_fin_ult1        8037851\n",
       "ind_valo_fin_ult1        8037851\n",
       "ind_viv_fin_ult1         8037851\n",
       "ind_nomina_ult1          8037851\n",
       "ind_nom_pens_ult1        8037851\n",
       "ind_recibo_ult1          8037851\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Machine learning pipeline Implementation\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any previous spark session and check df file type\n",
    "spark.stop()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark SQL dataframe \n",
    "## IMPORTANT!! - this cell usually takes time due to data volume!!!\n",
    "## IMPORTANT!! - Only run this cell once! (to run it again, you need to restart the kernel)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlCtx = SQLContext(sc) #print(sc)\n",
    "df_spark = sqlCtx.createDataFrame(df)\n",
    "type(df_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define datatypes in dataframe\n",
    "\n",
    "df_spark = df_spark.select(df_spark.fecha_dato.cast(\"date\"),\n",
    "                                   df_spark.ncodpers.cast(\"float\"),\n",
    "                                   df_spark.ind_empleado.cast(\"string\"),\n",
    "                                   df_spark.pais_residencia.cast(\"string\"),\n",
    "                                   df_spark.sexo.cast(\"string\"),\n",
    "                                   df_spark.age.cast(\"float\"),\n",
    "                                   df_spark.fecha_alta.cast(\"date\"),\n",
    "                                   df_spark.ind_nuevo.cast(\"float\"),\n",
    "                                   df_spark.antiguedad.cast(\"float\"),\n",
    "                                   df_spark.indrel.cast(\"float\"),\n",
    "                                   df_spark.indrel_1mes.cast(\"float\"),\n",
    "                                   df_spark.tiprel_1mes.cast(\"string\"),\n",
    "                                   df_spark.indresi.cast(\"string\"),\n",
    "                                   df_spark.indext.cast(\"string\"),\n",
    "                                   df_spark.canal_entrada.cast(\"string\"),\n",
    "                                   df_spark.indfall.cast(\"string\"),\n",
    "                                   df_spark.nomprov.cast(\"string\"),\n",
    "                                   df_spark.ind_actividad_cliente.cast(\"float\"),\n",
    "                                   df_spark.renta.cast(\"float\"),\n",
    "                                   df_spark.segmento.cast(\"string\"),\n",
    "                                   df_spark.ind_ahor_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_aval_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cder_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cno_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctju_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctma_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctop_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctpp_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deme_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_dela_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ecue_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_fond_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_hip_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_plan_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_pres_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_reca_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_tjcr_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_valo_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_viv_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nomina_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nom_pens_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_recibo_ult1.cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# IMPORTANT - Define target label (for prediction) from target features. Target select = mortgage products\n",
    "labels = \"ind_hip_fin_ult1\"  \n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "    \n",
    "# define variables; categorical, countinuous and target features\n",
    "\n",
    "numericCols = [\"age\",\"antiguedad\",\"renta\"]\n",
    "\n",
    "categoricalColumns = [\"ind_empleado\",\"pais_residencia\",\"sexo\",\"ind_nuevo\",\"indrel\", \n",
    "                      \"indrel_1mes\",\"tiprel_1mes\", \"indresi\", \"indext\", \"canal_entrada\",\"nomprov\", \n",
    "                      \"ind_actividad_cliente\",\"segmento\"]\n",
    "\n",
    "targetsColumns = [\"ind_ahor_fin_ult1\", \"ind_aval_fin_ult1\",\n",
    "                        \"ind_cco_fin_ult1\", \"ind_cder_fin_ult1\", \"ind_cno_fin_ult1\",\n",
    "                        \"ind_ctma_fin_ult1\", \"ind_ctop_fin_ult1\",\n",
    "                        \"ind_ctpp_fin_ult1\", \"ind_deco_fin_ult1\", \"ind_deme_fin_ult1\", \n",
    "                        \"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\", \"ind_fond_fin_ult1\",\n",
    "                        \"ind_ctju_fin_ult1\", \"ind_plan_fin_ult1\", \"ind_pres_fin_ult1\",\n",
    "                        \"ind_reca_fin_ult1\", \"ind_tjcr_fin_ult1\", \"ind_valo_fin_ult1\", \n",
    "                        \"ind_viv_fin_ult1\", \"ind_nomina_ult1\", \"ind_nom_pens_ult1\",\"ind_recibo_ult1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\") # Category Indexing with StringIndexer\n",
    "    stages += [stringIndexer]  # Add stages to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define categorical index columns \n",
    "categoricalColumnsIDX = [\"ind_empleadoIndex\",\"pais_residenciaIndex\",\"sexoIndex\",\n",
    "                         \"ind_nuevoIndex\",\"indrelIndex\",\"indrel_1mesIndex\",\n",
    "                         \"tiprel_1mesIndex\",\"indresiIndex\",\"indextIndex\", \n",
    "                         \"canal_entradaIndex\",\"nomprovIndex\",\"ind_actividad_clienteIndex\",\"segmentoIndex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = labels,\n",
    "                                outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "assemblerInputs = categoricalColumnsIDX + numericCols + targetsColumns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs,\n",
    "                            outputCol = \"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prePipeline = Pipeline(stages = stages)\n",
    "pipelineModel = prePipeline.fit(df_spark)\n",
    "\n",
    "dataset = pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol = \"label\", \n",
    "                            featuresCol = \"features\", \n",
    "                            numTrees = 100,                 #  Number of trees in the random forest\n",
    "                            impurity = 'entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy = \"auto\",\n",
    "                            predictionCol = \"prediction\",\n",
    "                            maxDepth = 5, \n",
    "                            maxBins = 50, \n",
    "                            minInstancesPerNode = 2) \n",
    "                            #minInfoGain=0.0, \n",
    "                            #subsamplingRate=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section C) Evaluation of Performance and testing\n",
    "Evaluate the performance of your pipeline using training and test set (don’t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set\n",
    "[trainData, testData] = dataset.randomSplit([0.8, 0.2], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "# evaluation of model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", \n",
    "                                              predictionCol = \"prediction\", \n",
    "                                              metricName = \"accuracy\")\n",
    "# random forest parameters\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "# Method: pyspark.ml.tuning.TrainValidationSplit\n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting cross-validation')\n",
    "cvModel = crossval.fit(trainData)  # This takes time!\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.999067\n",
      "Training Error = 0.00093345\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(trainData)\n",
    "train_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Training Accuracy = %g\" % (train_accuracy))\n",
    "print(\"Training Error = %g\" % (1.0 - train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.998105\n",
      "Test Error = 0.00189452\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % (test_accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section D) Implement a parameter grid - Model fine-tuning \n",
    "\n",
    "### Note: This section takes long time to compute!\n",
    "\n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Evaluate model performance using a subset of preprocessing variables\n",
    "#### No numeric predictors used, relaunch pipeline with this new preprocessing structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New preprocessing stage, without numeric predictors\n",
    "new_stages = []\n",
    "\n",
    "# remove preprocessing numeric predictors by including an empty vector\n",
    "New_numericCols = [] # empty numeric predictors\n",
    "\n",
    "# Add Newstages to the pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\")\n",
    "    new_stages += [stringIndexer]  # Add stages to the pipeline\n",
    "\n",
    "new_stages += [label_stringIdx]\n",
    "\n",
    "# empty vector is inserted here\n",
    "new_assemblerInputs = categoricalColumnsIDX + New_numericCols + targetsColumns\n",
    "new_assembler = VectorAssembler(inputCols = new_assemblerInputs, outputCol = \"features\")\n",
    "\n",
    "new_stages += [new_assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating new pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "new_prePipeline = Pipeline(stages = new_stages)\n",
    "new_pipelineModel = new_prePipeline.fit(df_spark)\n",
    "\n",
    "new_dataset = new_pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.999067\n",
      "Training Error = 0.00093345\n",
      "Test Accuracy = 0.998105\n",
      "Test Error = 0.00189452\n"
     ]
    }
   ],
   "source": [
    "[new_trainData, new_testData] = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "new_cvModel = crossval.fit(new_trainData)  # This takes time!\n",
    "\n",
    "# Results:\n",
    "\n",
    "new_predictions = cvModel.transform(new_trainData)\n",
    "new_train_accuracy = evaluator.evaluate(new_predictions)\n",
    "print(\"New Training Accuracy = %g\" % (new_train_accuracy))\n",
    "print(\"New Training Error = %g\" % (1.0 - new_train_accuracy))\n",
    "\n",
    "new_test_predictions = cvModel.transform(new_testData)\n",
    "new_test_accuracy = evaluator.evaluate(new_test_predictions)\n",
    "print(\"New Test Accuracy = %g\" % (new_test_accuracy))\n",
    "print(\"New Test Error = %g\" % (1.0 - new_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size evaluation\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n",
      "\n",
      "\n",
      "=== training set of size 100%, wait please\n",
      "Classification Error = 0.00093345\n"
     ]
    }
   ],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "%time\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.1, 0.01, 0.001, 0.0001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n\\n=== training set of size 100%, wait please')\n",
    "cvModel = crossval.fit(trainData)\n",
    "predictions = cvModel.transform(trainData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.5 µs\n",
      "\n",
      "\n",
      "=== training set of size reduced to 50.0%, wait please\n",
      "Classification Error = 0.00301217\n",
      "\n",
      "\n",
      "=== training set of size reduced to 10.0%, wait please\n",
      "Classification Error = 0.00729853\n",
      "\n",
      "\n",
      "=== training set of size reduced to 5.0%, wait please\n",
      "Classification Error = 0.00140168\n",
      "\n",
      "\n",
      "=== training set of size reduced to 1.0%, wait please\n",
      "Classification Error = 0.00433839\n",
      "\n",
      "\n",
      "=== training set of size reduced to 0.1%, wait please\n",
      "Classification Error = 0.00980392\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "i = 0\n",
    "for split in data:\n",
    "    print('\\n\\n=== training set of size reduced to {}%, wait please'.format(sizes[i]*100))\n",
    "    cvModel = crossval.fit(split)\n",
    "    predictions = cvModel.transform(split)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - accuracy))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 µs\n",
      "starting Hyperparameter Grid Search with cross-validation\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "%time\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,100,500]) \\\n",
    "    .addGrid(rf.maxDepth, [2,10]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "cvModel = crossval.fit(trainData)\n",
    "print('Grid Search with cross-validation has finished')\n",
    "\n",
    "# pick best model\n",
    "rfModel = cvModel.bestModel\n",
    "print (rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = rfModel.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Findings and conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As expected, Random Forest is able to return a good generalisation accuracy (validated on the Test Data). The average classification accuracy on the Test Data across different models tested is 99%. The level of abstraction and flexibility offer by the RF model allows for a excellent accuracy results even when the number of predictors is reduced, as seen in section 4.1. This process resulted in a simpler model yet equally powerful.\n",
    "\n",
    "As expected, the model is sensitive to the reduction in the size of the training data set. The more data the better the model. More training data reduces the effect of outliers and increases the generalisation accuracy of the final model. Besides, more data is likely to reduce the effect of bias in the data. As seen in section 4.2, the accuracy can vary widely depending on randomness selecting training/validation data. Less data makes more likely selecting a no representative sample for training the model, or for the evaluation. \n",
    "\n",
    "The grid search performed as part of this analysis shows that a RF containing more small trees (low depth) provided better results with less computational cost than a smaller number of deeper trees. Depth trees increase computational cost exponentially and have greater risk of overfitting, without a significant gain in performance (section 4.3).\n",
    "\n",
    "In summary, Random Forest is a valid approach to perform classification predictions given the nature of the data, large, complex, non-linear and non-uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
