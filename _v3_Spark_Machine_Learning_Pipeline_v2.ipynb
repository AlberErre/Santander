{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Summary of machine learning pipeline\n",
    "Step 1.  \n",
    "Step 2.  \n",
    "Step 3.  \n",
    "Step 4.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type_dict={'ncodpers':np.int32,\n",
    "           'ind_ahor_fin_ult1':np.uint8, 'ind_aval_fin_ult1':np.uint8, \n",
    "           'ind_cco_fin_ult1':np.uint8,'ind_cder_fin_ult1':np.uint8,\n",
    "           'ind_cno_fin_ult1':np.uint8,'ind_ctju_fin_ult1':np.uint8,'ind_ctma_fin_ult1':np.uint8,\n",
    "           'ind_ctop_fin_ult1':np.uint8,'ind_ctpp_fin_ult1':np.uint8,'ind_deco_fin_ult1':np.uint8,\n",
    "           'ind_deme_fin_ult1':np.uint8,'ind_dela_fin_ult1':np.uint8,'ind_ecue_fin_ult1':np.uint8,\n",
    "           'ind_fond_fin_ult1':np.uint8,'ind_hip_fin_ult1':np.uint8,'ind_plan_fin_ult1':np.uint8,\n",
    "           'ind_pres_fin_ult1':np.uint8,'ind_reca_fin_ult1':np.uint8,'ind_tjcr_fin_ult1':np.uint8,\n",
    "           'ind_valo_fin_ult1':np.uint8,'ind_viv_fin_ult1':np.uint8, 'ind_recibo_ult1':np.uint8 }\n",
    "\n",
    "# load data from server into dataframe (only loading the top 1,000,000 for demonstration purpose)\n",
    "df=pd.read_csv(\"/data/tempstore/santander-products/train_ver2.csv\", nrows=1000000, dtype=type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>indrel</th>\n",
       "      <th>indrel_1mes</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>cod_prov</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>renta</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.0</td>\n",
       "      <td>982266.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>8.248170e+05</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>994598.000000</td>\n",
       "      <td>994598.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.905967e+05</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>1.109074</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.852131</td>\n",
       "      <td>0.564971</td>\n",
       "      <td>1.396462e+05</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.039378</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.071629</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.166275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.044084e+05</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>3.267624</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.422924</td>\n",
       "      <td>0.495761</td>\n",
       "      <td>2.389858e+05</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099410</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>0.068112</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>0.248429</td>\n",
       "      <td>0.194493</td>\n",
       "      <td>0.080003</td>\n",
       "      <td>0.257873</td>\n",
       "      <td>0.270584</td>\n",
       "      <td>0.372327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.588900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.202730e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.364110e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.157184e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.644760e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.066519e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.074511e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.634325e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.379131e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.889440e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ncodpers      ind_nuevo         indrel    indrel_1mes   tipodom  \\\n",
       "count  1.000000e+06  989218.000000  989218.000000  989218.000000  989218.0   \n",
       "mean   6.905967e+05       0.000489       1.109074       1.000085       1.0   \n",
       "std    4.044084e+05       0.022114       3.267624       0.012954       0.0   \n",
       "min    1.588900e+04       0.000000       1.000000       1.000000       1.0   \n",
       "25%    3.364110e+05       0.000000       1.000000       1.000000       1.0   \n",
       "50%    6.644760e+05       0.000000       1.000000       1.000000       1.0   \n",
       "75%    1.074511e+06       0.000000       1.000000       1.000000       1.0   \n",
       "max    1.379131e+06       1.000000      99.000000       3.000000       1.0   \n",
       "\n",
       "            cod_prov  ind_actividad_cliente         renta  ind_ahor_fin_ult1  \\\n",
       "count  982266.000000          989218.000000  8.248170e+05     1000000.000000   \n",
       "mean       26.852131               0.564971  1.396462e+05           0.000177   \n",
       "std        12.422924               0.495761  2.389858e+05           0.013303   \n",
       "min         1.000000               0.000000  1.202730e+03           0.000000   \n",
       "25%        18.000000               0.000000  7.157184e+04           0.000000   \n",
       "50%        28.000000               1.000000  1.066519e+05           0.000000   \n",
       "75%        33.000000               1.000000  1.634325e+05           0.000000   \n",
       "max        52.000000               1.000000  2.889440e+07           1.000000   \n",
       "\n",
       "       ind_aval_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "count     1000000.000000       ...           1000000.000000   \n",
       "mean            0.000039       ...                 0.009982   \n",
       "std             0.006245       ...                 0.099410   \n",
       "min             0.000000       ...                 0.000000   \n",
       "25%             0.000000       ...                 0.000000   \n",
       "50%             0.000000       ...                 0.000000   \n",
       "75%             0.000000       ...                 0.000000   \n",
       "max             1.000000       ...                 1.000000   \n",
       "\n",
       "       ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "count     1000000.000000     1000000.000000     1000000.000000   \n",
       "mean            0.014553           0.004661           0.072581   \n",
       "std             0.119755           0.068112           0.259448   \n",
       "min             0.000000           0.000000           0.000000   \n",
       "25%             0.000000           0.000000           0.000000   \n",
       "50%             0.000000           0.000000           0.000000   \n",
       "75%             0.000000           0.000000           0.000000   \n",
       "max             1.000000           1.000000           1.000000   \n",
       "\n",
       "       ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "count     1000000.000000     1000000.000000    1000000.000000   \n",
       "mean            0.066084           0.039378          0.006442   \n",
       "std             0.248429           0.194493          0.080003   \n",
       "min             0.000000           0.000000          0.000000   \n",
       "25%             0.000000           0.000000          0.000000   \n",
       "50%             0.000000           0.000000          0.000000   \n",
       "75%             0.000000           0.000000          0.000000   \n",
       "max             1.000000           1.000000          1.000000   \n",
       "\n",
       "       ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "count    994598.000000      994598.000000   1000000.000000  \n",
       "mean          0.071629           0.079543         0.166275  \n",
       "std           0.257873           0.270584         0.372327  \n",
       "min           0.000000           0.000000         0.000000  \n",
       "25%           0.000000           0.000000         0.000000  \n",
       "50%           0.000000           0.000000         0.000000  \n",
       "75%           0.000000           0.000000         0.000000  \n",
       "max           1.000000           1.000000         1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               1000000\n",
       "ncodpers                 1000000\n",
       "ind_empleado              989218\n",
       "pais_residencia           989218\n",
       "sexo                      989214\n",
       "age                      1000000\n",
       "fecha_alta                989218\n",
       "ind_nuevo                 989218\n",
       "antiguedad               1000000\n",
       "indrel                    989218\n",
       "ult_fec_cli_1t              1101\n",
       "indrel_1mes               989218\n",
       "tiprel_1mes               989218\n",
       "indresi                   989218\n",
       "indext                    989218\n",
       "conyuemp                     178\n",
       "canal_entrada             989139\n",
       "indfall                   989218\n",
       "tipodom                   989218\n",
       "cod_prov                  982266\n",
       "nomprov                   982266\n",
       "ind_actividad_cliente     989218\n",
       "renta                     824817\n",
       "segmento                  989105\n",
       "ind_ahor_fin_ult1        1000000\n",
       "ind_aval_fin_ult1        1000000\n",
       "ind_cco_fin_ult1         1000000\n",
       "ind_cder_fin_ult1        1000000\n",
       "ind_cno_fin_ult1         1000000\n",
       "ind_ctju_fin_ult1        1000000\n",
       "ind_ctma_fin_ult1        1000000\n",
       "ind_ctop_fin_ult1        1000000\n",
       "ind_ctpp_fin_ult1        1000000\n",
       "ind_deco_fin_ult1        1000000\n",
       "ind_deme_fin_ult1        1000000\n",
       "ind_dela_fin_ult1        1000000\n",
       "ind_ecue_fin_ult1        1000000\n",
       "ind_fond_fin_ult1        1000000\n",
       "ind_hip_fin_ult1         1000000\n",
       "ind_plan_fin_ult1        1000000\n",
       "ind_pres_fin_ult1        1000000\n",
       "ind_reca_fin_ult1        1000000\n",
       "ind_tjcr_fin_ult1        1000000\n",
       "ind_valo_fin_ult1        1000000\n",
       "ind_viv_fin_ult1         1000000\n",
       "ind_nomina_ult1           994598\n",
       "ind_nom_pens_ult1         994598\n",
       "ind_recibo_ult1          1000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only unique id\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "df = df[df.ncodpers.isin(unique_ids)]  \n",
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# eliminate mostly empty columns and redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\", \"ult_fec_cli_1t\",\"conyuemp\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform to numeric and set missing values to nan\n",
    "df['age']=pd.to_numeric(df.age, errors='coerce')\n",
    "df['ind_nuevo']=pd.to_numeric(df.ind_nuevo, errors='coerce')\n",
    "df['antiguedad']=pd.to_numeric(df.antiguedad, errors='coerce')\n",
    "df['indrel']=pd.to_numeric(df.indrel, errors='coerce')\n",
    "df['renta']=pd.to_numeric(df.renta, errors='coerce')\n",
    "df['indrel_1mes']=pd.to_numeric(df.indrel_1mes, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from age variable\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-28T00:00:00.000000000', '2015-02-28T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfor dates to datetime datatype\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fill datetime missing values\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado              True\n",
       "pais_residencia           True\n",
       "sexo                      True\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                 True\n",
       "antiguedad                True\n",
       "indrel                    True\n",
       "indrel_1mes               True\n",
       "tiprel_1mes               True\n",
       "indresi                   True\n",
       "indext                    True\n",
       "canal_entrada             True\n",
       "indfall                   True\n",
       "nomprov                   True\n",
       "ind_actividad_cliente     True\n",
       "renta                     True\n",
       "segmento                  True\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1           True\n",
       "ind_nom_pens_ult1         True\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace other missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove rows with any nan value left\n",
    "df = df.dropna(subset=['renta', 'segmento', 'canal_entrada', 'ind_empleado', \n",
    "                       'pais_residencia', 'indresi', 'indresi', 'sexo'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado             False\n",
       "pais_residencia          False\n",
       "sexo                     False\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                False\n",
       "antiguedad               False\n",
       "indrel                   False\n",
       "indrel_1mes              False\n",
       "tiprel_1mes              False\n",
       "indresi                  False\n",
       "indext                   False\n",
       "canal_entrada            False\n",
       "indfall                  False\n",
       "nomprov                  False\n",
       "ind_actividad_cliente    False\n",
       "renta                    False\n",
       "segmento                 False\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1          False\n",
       "ind_nom_pens_ult1        False\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values are gone\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               824742\n",
       "ncodpers                 824742\n",
       "ind_empleado             824742\n",
       "pais_residencia          824742\n",
       "sexo                     824742\n",
       "age                      824742\n",
       "fecha_alta               824742\n",
       "ind_nuevo                824742\n",
       "antiguedad               824742\n",
       "indrel                   824742\n",
       "indrel_1mes              824742\n",
       "tiprel_1mes              824742\n",
       "indresi                  824742\n",
       "indext                   824742\n",
       "canal_entrada            824742\n",
       "indfall                  824742\n",
       "nomprov                  824742\n",
       "ind_actividad_cliente    824742\n",
       "renta                    824742\n",
       "segmento                 824742\n",
       "ind_ahor_fin_ult1        824742\n",
       "ind_aval_fin_ult1        824742\n",
       "ind_cco_fin_ult1         824742\n",
       "ind_cder_fin_ult1        824742\n",
       "ind_cno_fin_ult1         824742\n",
       "ind_ctju_fin_ult1        824742\n",
       "ind_ctma_fin_ult1        824742\n",
       "ind_ctop_fin_ult1        824742\n",
       "ind_ctpp_fin_ult1        824742\n",
       "ind_deco_fin_ult1        824742\n",
       "ind_deme_fin_ult1        824742\n",
       "ind_dela_fin_ult1        824742\n",
       "ind_ecue_fin_ult1        824742\n",
       "ind_fond_fin_ult1        824742\n",
       "ind_hip_fin_ult1         824742\n",
       "ind_plan_fin_ult1        824742\n",
       "ind_pres_fin_ult1        824742\n",
       "ind_reca_fin_ult1        824742\n",
       "ind_tjcr_fin_ult1        824742\n",
       "ind_valo_fin_ult1        824742\n",
       "ind_viv_fin_ult1         824742\n",
       "ind_nomina_ult1          824742\n",
       "ind_nom_pens_ult1        824742\n",
       "ind_recibo_ult1          824742\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               datetime64[ns]\n",
       "ncodpers                          int32\n",
       "ind_empleado                     object\n",
       "pais_residencia                  object\n",
       "sexo                             object\n",
       "age                               int64\n",
       "fecha_alta               datetime64[ns]\n",
       "ind_nuevo                       float64\n",
       "antiguedad                      float64\n",
       "indrel                          float64\n",
       "indrel_1mes                    category\n",
       "tiprel_1mes                    category\n",
       "indresi                          object\n",
       "indext                           object\n",
       "canal_entrada                    object\n",
       "indfall                          object\n",
       "nomprov                          object\n",
       "ind_actividad_cliente           float64\n",
       "renta                           float64\n",
       "segmento                         object\n",
       "ind_ahor_fin_ult1                 uint8\n",
       "ind_aval_fin_ult1                 uint8\n",
       "ind_cco_fin_ult1                  uint8\n",
       "ind_cder_fin_ult1                 uint8\n",
       "ind_cno_fin_ult1                  uint8\n",
       "ind_ctju_fin_ult1                 uint8\n",
       "ind_ctma_fin_ult1                 uint8\n",
       "ind_ctop_fin_ult1                 uint8\n",
       "ind_ctpp_fin_ult1                 uint8\n",
       "ind_deco_fin_ult1                 uint8\n",
       "ind_deme_fin_ult1                 uint8\n",
       "ind_dela_fin_ult1                 uint8\n",
       "ind_ecue_fin_ult1                 uint8\n",
       "ind_fond_fin_ult1                 uint8\n",
       "ind_hip_fin_ult1                  uint8\n",
       "ind_plan_fin_ult1                 uint8\n",
       "ind_pres_fin_ult1                 uint8\n",
       "ind_reca_fin_ult1                 uint8\n",
       "ind_tjcr_fin_ult1                 uint8\n",
       "ind_valo_fin_ult1                 uint8\n",
       "ind_viv_fin_ult1                  uint8\n",
       "ind_nomina_ult1                 float64\n",
       "ind_nom_pens_ult1               float64\n",
       "ind_recibo_ult1                   uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlCtx = SQLContext(sc) #print(sc)\n",
    "df_spark = sqlCtx.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fecha_dato', 'bigint'),\n",
       " ('ncodpers', 'bigint'),\n",
       " ('ind_empleado', 'string'),\n",
       " ('pais_residencia', 'string'),\n",
       " ('sexo', 'string'),\n",
       " ('age', 'bigint'),\n",
       " ('fecha_alta', 'bigint'),\n",
       " ('ind_nuevo', 'double'),\n",
       " ('antiguedad', 'double'),\n",
       " ('indrel', 'double'),\n",
       " ('indrel_1mes', 'string'),\n",
       " ('tiprel_1mes', 'string'),\n",
       " ('indresi', 'string'),\n",
       " ('indext', 'string'),\n",
       " ('canal_entrada', 'string'),\n",
       " ('indfall', 'string'),\n",
       " ('nomprov', 'string'),\n",
       " ('ind_actividad_cliente', 'double'),\n",
       " ('renta', 'double'),\n",
       " ('segmento', 'string'),\n",
       " ('ind_ahor_fin_ult1', 'bigint'),\n",
       " ('ind_aval_fin_ult1', 'bigint'),\n",
       " ('ind_cco_fin_ult1', 'bigint'),\n",
       " ('ind_cder_fin_ult1', 'bigint'),\n",
       " ('ind_cno_fin_ult1', 'bigint'),\n",
       " ('ind_ctju_fin_ult1', 'bigint'),\n",
       " ('ind_ctma_fin_ult1', 'bigint'),\n",
       " ('ind_ctop_fin_ult1', 'bigint'),\n",
       " ('ind_ctpp_fin_ult1', 'bigint'),\n",
       " ('ind_deco_fin_ult1', 'bigint'),\n",
       " ('ind_deme_fin_ult1', 'bigint'),\n",
       " ('ind_dela_fin_ult1', 'bigint'),\n",
       " ('ind_ecue_fin_ult1', 'bigint'),\n",
       " ('ind_fond_fin_ult1', 'bigint'),\n",
       " ('ind_hip_fin_ult1', 'bigint'),\n",
       " ('ind_plan_fin_ult1', 'bigint'),\n",
       " ('ind_pres_fin_ult1', 'bigint'),\n",
       " ('ind_reca_fin_ult1', 'bigint'),\n",
       " ('ind_tjcr_fin_ult1', 'bigint'),\n",
       " ('ind_valo_fin_ult1', 'bigint'),\n",
       " ('ind_viv_fin_ult1', 'bigint'),\n",
       " ('ind_nomina_ult1', 'double'),\n",
       " ('ind_nom_pens_ult1', 'double'),\n",
       " ('ind_recibo_ult1', 'bigint')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_spark.describe\n",
    "df_spark.dtypes\n",
    "#df_spark.take(2)\n",
    "#df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.select(df_spark.fecha_dato.cast(\"date\"),\n",
    "                                   df_spark.ncodpers.cast(\"float\"),\n",
    "                                   df_spark.ind_empleado.cast(\"string\"),\n",
    "                                   df_spark.pais_residencia.cast(\"string\"),\n",
    "                                   df_spark.sexo.cast(\"string\"),\n",
    "                                   df_spark.age.cast(\"float\"),\n",
    "                                   df_spark.fecha_alta.cast(\"date\"),\n",
    "                                   df_spark.ind_nuevo.cast(\"float\"),\n",
    "                                   df_spark.antiguedad.cast(\"float\"),\n",
    "                                   df_spark.indrel.cast(\"float\"),\n",
    "                                   df_spark.indrel_1mes.cast(\"float\"),\n",
    "                                   df_spark.tiprel_1mes.cast(\"string\"),\n",
    "                                   df_spark.indresi.cast(\"string\"),\n",
    "                                   df_spark.indext.cast(\"string\"),\n",
    "                                   df_spark.canal_entrada.cast(\"string\"),\n",
    "                                   df_spark.indfall.cast(\"string\"),\n",
    "                                   df_spark.nomprov.cast(\"string\"),\n",
    "                                   df_spark.ind_actividad_cliente.cast(\"float\"),\n",
    "                                   df_spark.renta.cast(\"float\"),\n",
    "                                   df_spark.segmento.cast(\"string\"),\n",
    "                                   df_spark.ind_ahor_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_aval_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cder_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cno_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctju_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctma_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctop_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctpp_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deme_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_dela_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ecue_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_fond_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_hip_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_plan_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_pres_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_reca_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_tjcr_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_valo_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_viv_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nomina_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nom_pens_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_recibo_ult1.cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fecha_dato', 'date'),\n",
       " ('ncodpers', 'float'),\n",
       " ('ind_empleado', 'string'),\n",
       " ('pais_residencia', 'string'),\n",
       " ('sexo', 'string'),\n",
       " ('age', 'float'),\n",
       " ('fecha_alta', 'date'),\n",
       " ('ind_nuevo', 'float'),\n",
       " ('antiguedad', 'float'),\n",
       " ('indrel', 'float'),\n",
       " ('indrel_1mes', 'float'),\n",
       " ('tiprel_1mes', 'string'),\n",
       " ('indresi', 'string'),\n",
       " ('indext', 'string'),\n",
       " ('canal_entrada', 'string'),\n",
       " ('indfall', 'string'),\n",
       " ('nomprov', 'string'),\n",
       " ('ind_actividad_cliente', 'float'),\n",
       " ('renta', 'float'),\n",
       " ('segmento', 'string'),\n",
       " ('ind_ahor_fin_ult1', 'float'),\n",
       " ('ind_aval_fin_ult1', 'float'),\n",
       " ('ind_cco_fin_ult1', 'float'),\n",
       " ('ind_cder_fin_ult1', 'float'),\n",
       " ('ind_cno_fin_ult1', 'float'),\n",
       " ('ind_ctju_fin_ult1', 'float'),\n",
       " ('ind_ctma_fin_ult1', 'float'),\n",
       " ('ind_ctop_fin_ult1', 'float'),\n",
       " ('ind_ctpp_fin_ult1', 'float'),\n",
       " ('ind_deco_fin_ult1', 'float'),\n",
       " ('ind_deme_fin_ult1', 'float'),\n",
       " ('ind_dela_fin_ult1', 'float'),\n",
       " ('ind_ecue_fin_ult1', 'float'),\n",
       " ('ind_fond_fin_ult1', 'float'),\n",
       " ('ind_hip_fin_ult1', 'float'),\n",
       " ('ind_plan_fin_ult1', 'float'),\n",
       " ('ind_pres_fin_ult1', 'float'),\n",
       " ('ind_reca_fin_ult1', 'float'),\n",
       " ('ind_tjcr_fin_ult1', 'float'),\n",
       " ('ind_valo_fin_ult1', 'float'),\n",
       " ('ind_viv_fin_ult1', 'float'),\n",
       " ('ind_nomina_ult1', 'float'),\n",
       " ('ind_nom_pens_ult1', 'float'),\n",
       " ('ind_recibo_ult1', 'float')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"inputCols\". Could not convert fecha_datoclassVec to list of strings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mtoListString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTypeConverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not convert %s to list of strings\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert fecha_datoclassVec to list of strings",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-719ac20df330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"classVec\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use OneHotEncoder to convert categorical variables into binary SparseVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mstages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"classVec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mstages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Add stage to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputCols, outputCol)\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.ml.feature.VectorAssembler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36msetParams\u001b[0;34m(self, inputCols, outputCol)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \"\"\"\n\u001b[1;32m   1880\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"inputCols\". Could not convert fecha_datoclassVec to list of strings"
     ]
    }
   ],
   "source": [
    "# Example implementation from apache official documentation:\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if 1 == 1:\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"RandomForestClassifierExample\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    \n",
    "    stages = []\n",
    "\n",
    "    inputCol =  [\"fecha_dato\", \"ncodpers\", \"ind_empleado\", \"pais_residencia\",\"sexo\",\n",
    "            \"age\",\"fecha_alta\",\"ind_nuevo\",\"antiguedad\", \"indrel\", \"indrel_1mes\",\n",
    "            \"tiprel_1mes\", \"indresi\", \"indext\", \"canal_entrada\",\n",
    "            \"indfall\", \"nomprov\", \"ind_actividad_cliente\", \"renta\",\n",
    "            \"segmento\", \"ind_ahor_fin_ult1\", \"ind_aval_fin_ult1\",\n",
    "            \"ind_cco_fin_ult1\", \"ind_cder_fin_ult1\", \"ind_cno_fin_ult1\",\n",
    "            \"ind_ctju_fin_ult1\", \"ind_ctma_fin_ult1\", \"ind_ctop_fin_ult1\",\n",
    "            \"ind_ctpp_fin_ult1\", \"ind_deco_fin_ult1\", \"ind_deme_fin_ult1\", \n",
    "            \"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\", \"ind_fond_fin_ult1\",\n",
    "            \"ind_hip_fin_ult1\", \"ind_plan_fin_ult1\", \"ind_pres_fin_ult1\",\n",
    "            \"ind_reca_fin_ult1\", \"ind_tjcr_fin_ult1\", \"ind_valo_fin_ult1\", \n",
    "            \"ind_viv_fin_ult1\", \"ind_nomina_ult1\", \"ind_nom_pens_ult1\",\"ind_recibo_ult1\"]\n",
    "    \n",
    "    for _col in inputCol:\n",
    "        stringIndexer = StringIndexer(inputCol=_col, outputCol=_col+\"Index\") # Category Indexing with StringIndexer\n",
    "        encoder = OneHotEncoder(inputCol=_col+\"Index\", outputCol=_col+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "        stages += [stringIndexer, encoder]\n",
    "        assembler = VectorAssembler(inputCols=_col+\"classVec\", outputCol=_col+\"feature\")\n",
    "        stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "    stages += [rf]  # Add stage to the pipeline\n",
    "\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'map' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1d66292205a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Transform all features into a vector using VectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnumericCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0massemblerInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"classVec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategoricalColumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumericCols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massemblerInputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Add stage to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'map' and 'list'"
     ]
    }
   ],
   "source": [
    "## ORIGINAL CODE from internet\n",
    "\n",
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "\n",
    "# One-Hot Encoding\n",
    "categoricalColumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"j\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\") # Category Indexing with StringIndexer\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    stages += [stringIndexer, encoder]  # Add stages to the pipeline\n",
    "    \n",
    "# Convert labels into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"add here target column in csv file\", outputCol = \"labels\")\n",
    "stages += [label_stringIdx]  # Add stage to the pipeline\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"m\", \"n\", \"o\", \"p\", \"q\", \"r\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"labels\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=100,                 #  Number of trees in the random forest\n",
    "                            impurity='entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy=\"auto\",\n",
    "                            predictionCol=\"prediction\")\n",
    "                            #maxDepth=5, \n",
    "                            #maxBins=32, \n",
    "                            #minInstancesPerNode=1, \n",
    "                            #minInfoGain=0.0, \n",
    "                            #subsamplingRate=1.0)\n",
    "stages += [rf]  # Add stage to the pipeline\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (don’t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split data into training set and testing set\n",
    "[trainData, testData] = trainData.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "# Train model in pipeline\n",
    "rfModel = pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "predictions = rfModel.transform(trainData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(train_pipeline.stages[0])  # summary\n",
    "\n",
    "\n",
    "# Run the feature transformations pipeline on the test data set\n",
    "pipelineModel = prePro_pipeline.fit(testClients)  #  computes feature statistics\n",
    "testData = pipelineModel.transform(testClients)  #  transforms the features\n",
    "\n",
    "# Make predictions for test set and compute test error\n",
    "test_predictions = rfModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Model fine-tuning. (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n=== training set of size 100%')\n",
    "# Train model in pipeline\n",
    "tempModel = pipeline.fit(trainData)\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "tempPredictions = tempModel.transform(trainData)\n",
    "tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "\n",
    "for x in data:\n",
    "    print('\\n=== training set of size reduced to %g' % x)\n",
    "    # Train model in pipeline\n",
    "    tempModel = pipeline.fit(data[x])\n",
    "    # Make predictions for training set and compute training set accuracy\n",
    "    tempPredictions = tempModel.transform(data[x])\n",
    "    tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,20,50,100,200,500,1000,5000]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [0,1,2,4,6,8,10]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,10,20,50]).build()\n",
    "\n",
    "# Grid Search and Cross Validation\n",
    "crossVal = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "rfCrosVal = crossVal.fit(trainData)\n",
    "print('Grid Search has finished')\n",
    "\n",
    "print(rfCrosVal.bestModel.rank)\n",
    "paramMap = list(zip(rfCrosVal.getEstimatorParamMaps(),rfCrosVal.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the model with test data\n",
    "cvtest_predictions = rfCrosVal.transform(testData)\n",
    "cvtest_accuracy = evaluator.evaluate(cvtest_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - cvtest_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Evaluate model performance using a subset of variables (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
