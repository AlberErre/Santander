{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline - Task 2 (Big Data Module)\n",
    "\n",
    "Students:  \n",
    "Miguel Esteras & Alberto Ruiz Benitez de Lugo\n",
    "\n",
    "This coursework presents an implementation and application of Spark Machine Learning Pipelines. Evaluating them regarding preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A) Choice of dataset and task (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Santander products <-- DataSet\n",
    "\n",
    "We have selected \"Santander Products\" as our dataset. The reason of this choice has been the large amount of predictors and responses available within this data. This amount of possible variable combinations increases the complexity of the problem and therefore the selected model should has a deep level of abstraction when the amount of data is huge. \n",
    "Furthermore, this is a good example about why big data implementations, such as Spark, is worth to implement.\n",
    "\n",
    "Therefore, the high level of complexity of the data has been the main driver to choose \"Random Forest\" as our model for this pipeline. This method is currently state-of-the-art in many different Machine Learning Field, like computer vision. This model shows a very good performance in both, classification and regression implementations. Moreover, it presents a good level of abstraction, as commented above, that may provide good results with this complex and large dataset.\n",
    "\n",
    "### Task\n",
    "\n",
    "The goal (task) of this pipeline is to predict whether a financial product will be purchased by a consumer or not. Thus, given all the data available we are going to create a Random Forest model to predict when a particular Santander Product (we need to select which one) will be purchased by a customer or not. \n",
    "\n",
    "The method applied is estimating which combination of predictors (age, renta, ...etc) are more likely to select a particular nest of products. Then, we use this information to predict whether a particular customer would purchase a particular product or not. We need to select this product in advance, in our case we have selected \"ind_ctju_fin_ult1\" as our target product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B) Machine Learning Pipeline in Spark (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary of Pipeline\n",
    "\n",
    "- Load Data and first preprocessing (1.2)\n",
    "- Descriptive statistics (1.3)\n",
    "- Data Cleaning (1.4)\n",
    "- Machine learning pipeline Implementation using Random Forest (2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type_dict = {'ncodpers':np.int32,\n",
    "            'ind_ahor_fin_ult1':np.uint8, 'ind_aval_fin_ult1':np.uint8, \n",
    "            'ind_cco_fin_ult1':np.uint8,'ind_cder_fin_ult1':np.uint8,\n",
    "            'ind_cno_fin_ult1':np.uint8,'ind_ctju_fin_ult1':np.uint8,'ind_ctma_fin_ult1':np.uint8,\n",
    "            'ind_ctop_fin_ult1':np.uint8,'ind_ctpp_fin_ult1':np.uint8,'ind_deco_fin_ult1':np.uint8,\n",
    "            'ind_deme_fin_ult1':np.uint8,'ind_dela_fin_ult1':np.uint8,'ind_ecue_fin_ult1':np.uint8,\n",
    "            'ind_fond_fin_ult1':np.uint8,'ind_hip_fin_ult1':np.uint8,'ind_plan_fin_ult1':np.uint8,\n",
    "            'ind_pres_fin_ult1':np.uint8,'ind_reca_fin_ult1':np.uint8,'ind_tjcr_fin_ult1':np.uint8,\n",
    "            'ind_valo_fin_ult1':np.uint8,'ind_viv_fin_ult1':np.uint8, 'ind_recibo_ult1':np.uint8 }\n",
    "\n",
    "# load data from server into dataframe (only loading the top 100,000 for demonstration purpose)\n",
    "df = pd.read_csv(\"/data/tempstore/santander-products/train_ver2.csv\",\n",
    "                 nrows = 100000,\n",
    "                 dtype = type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>indrel</th>\n",
       "      <th>indrel_1mes</th>\n",
       "      <th>conyuemp</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>cod_prov</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>renta</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>99317.000000</td>\n",
       "      <td>99317.000000</td>\n",
       "      <td>99317.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99317.0</td>\n",
       "      <td>99231.000000</td>\n",
       "      <td>99317.000000</td>\n",
       "      <td>8.171600e+04</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>99790.000000</td>\n",
       "      <td>99790.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.040513e+06</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>1.126303</td>\n",
       "      <td>1.000060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.944775</td>\n",
       "      <td>0.410302</td>\n",
       "      <td>1.155589e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.019250</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.035485</td>\n",
       "      <td>0.097760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.282306e+04</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>3.515940</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.646314</td>\n",
       "      <td>0.491891</td>\n",
       "      <td>1.594097e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.035474</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.137403</td>\n",
       "      <td>0.135251</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.178478</td>\n",
       "      <td>0.185002</td>\n",
       "      <td>0.296991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.885090e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.539800e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.019914e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.217089e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.055276e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.961021e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.088296e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.330239e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.375586e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.425324e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ncodpers     ind_nuevo        indrel   indrel_1mes  conyuemp  \\\n",
       "count  1.000000e+05  99317.000000  99317.000000  99317.000000       0.0   \n",
       "mean   1.040513e+06      0.000211      1.126303      1.000060       NaN   \n",
       "std    6.282306e+04      0.014540      3.515940      0.010992       NaN   \n",
       "min    8.885090e+05      0.000000      1.000000      1.000000       NaN   \n",
       "25%    1.019914e+06      0.000000      1.000000      1.000000       NaN   \n",
       "50%    1.055276e+06      0.000000      1.000000      1.000000       NaN   \n",
       "75%    1.088296e+06      0.000000      1.000000      1.000000       NaN   \n",
       "max    1.375586e+06      1.000000     99.000000      3.000000       NaN   \n",
       "\n",
       "       tipodom      cod_prov  ind_actividad_cliente         renta  \\\n",
       "count  99317.0  99231.000000           99317.000000  8.171600e+04   \n",
       "mean       1.0     24.944775               0.410302  1.155589e+05   \n",
       "std        0.0     13.646314               0.491891  1.594097e+05   \n",
       "min        1.0      1.000000               0.000000  2.539800e+03   \n",
       "25%        1.0     11.000000               0.000000  6.217089e+04   \n",
       "50%        1.0     28.000000               0.000000  8.961021e+04   \n",
       "75%        1.0     36.000000               1.000000  1.330239e+05   \n",
       "max        1.0     52.000000               1.000000  2.425324e+07   \n",
       "\n",
       "       ind_ahor_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "count           100000.0       ...            100000.000000   \n",
       "mean                 0.0       ...                 0.000110   \n",
       "std                  0.0       ...                 0.010488   \n",
       "min                  0.0       ...                 0.000000   \n",
       "25%                  0.0       ...                 0.000000   \n",
       "50%                  0.0       ...                 0.000000   \n",
       "75%                  0.0       ...                 0.000000   \n",
       "max                  0.0       ...                 1.000000   \n",
       "\n",
       "       ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "count      100000.000000      100000.000000      100000.000000   \n",
       "mean            0.001260           0.000200           0.019250   \n",
       "std             0.035474           0.014141           0.137403   \n",
       "min             0.000000           0.000000           0.000000   \n",
       "25%             0.000000           0.000000           0.000000   \n",
       "50%             0.000000           0.000000           0.000000   \n",
       "75%             0.000000           0.000000           0.000000   \n",
       "max             1.000000           1.000000           1.000000   \n",
       "\n",
       "       ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "count      100000.000000      100000.000000     100000.000000   \n",
       "mean            0.018640           0.004630          0.000060   \n",
       "std             0.135251           0.067887          0.007746   \n",
       "min             0.000000           0.000000          0.000000   \n",
       "25%             0.000000           0.000000          0.000000   \n",
       "50%             0.000000           0.000000          0.000000   \n",
       "75%             0.000000           0.000000          0.000000   \n",
       "max             1.000000           1.000000          1.000000   \n",
       "\n",
       "       ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "count     99790.000000       99790.000000    100000.000000  \n",
       "mean          0.032939           0.035485         0.097760  \n",
       "std           0.178478           0.185002         0.296991  \n",
       "min           0.000000           0.000000         0.000000  \n",
       "25%           0.000000           0.000000         0.000000  \n",
       "50%           0.000000           0.000000         0.000000  \n",
       "75%           0.000000           0.000000         0.000000  \n",
       "max           1.000000           1.000000         1.000000  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               100000\n",
       "ncodpers                 100000\n",
       "ind_empleado              99317\n",
       "pais_residencia           99317\n",
       "sexo                      99317\n",
       "age                      100000\n",
       "fecha_alta                99317\n",
       "ind_nuevo                 99317\n",
       "antiguedad               100000\n",
       "indrel                    99317\n",
       "ult_fec_cli_1t              128\n",
       "indrel_1mes               99317\n",
       "tiprel_1mes               99317\n",
       "indresi                   99317\n",
       "indext                    99317\n",
       "conyuemp                      0\n",
       "canal_entrada             99312\n",
       "indfall                   99317\n",
       "tipodom                   99317\n",
       "cod_prov                  99231\n",
       "nomprov                   99231\n",
       "ind_actividad_cliente     99317\n",
       "renta                     81716\n",
       "segmento                  99309\n",
       "ind_ahor_fin_ult1        100000\n",
       "ind_aval_fin_ult1        100000\n",
       "ind_cco_fin_ult1         100000\n",
       "ind_cder_fin_ult1        100000\n",
       "ind_cno_fin_ult1         100000\n",
       "ind_ctju_fin_ult1        100000\n",
       "ind_ctma_fin_ult1        100000\n",
       "ind_ctop_fin_ult1        100000\n",
       "ind_ctpp_fin_ult1        100000\n",
       "ind_deco_fin_ult1        100000\n",
       "ind_deme_fin_ult1        100000\n",
       "ind_dela_fin_ult1        100000\n",
       "ind_ecue_fin_ult1        100000\n",
       "ind_fond_fin_ult1        100000\n",
       "ind_hip_fin_ult1         100000\n",
       "ind_plan_fin_ult1        100000\n",
       "ind_pres_fin_ult1        100000\n",
       "ind_reca_fin_ult1        100000\n",
       "ind_tjcr_fin_ult1        100000\n",
       "ind_valo_fin_ult1        100000\n",
       "ind_viv_fin_ult1         100000\n",
       "ind_nomina_ult1           99790\n",
       "ind_nom_pens_ult1         99790\n",
       "ind_recibo_ult1          100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only unique id\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "df = df[df.ncodpers.isin(unique_ids)]  \n",
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# eliminate mostly empty columns and redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\", \"ult_fec_cli_1t\",\"conyuemp\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform to numeric and set missing values to nan\n",
    "df['age']=pd.to_numeric(df.age, errors='coerce')\n",
    "df['ind_nuevo']=pd.to_numeric(df.ind_nuevo, errors='coerce')\n",
    "df['antiguedad']=pd.to_numeric(df.antiguedad, errors='coerce')\n",
    "df['indrel']=pd.to_numeric(df.indrel, errors='coerce')\n",
    "df['renta']=pd.to_numeric(df.renta, errors='coerce')\n",
    "df['indrel_1mes']=pd.to_numeric(df.indrel_1mes, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from age variable\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-28T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfor dates to datetime datatype\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fill datetime missing values\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado              True\n",
       "pais_residencia           True\n",
       "sexo                      True\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                 True\n",
       "antiguedad                True\n",
       "indrel                    True\n",
       "indrel_1mes               True\n",
       "tiprel_1mes               True\n",
       "indresi                   True\n",
       "indext                    True\n",
       "canal_entrada             True\n",
       "indfall                   True\n",
       "nomprov                   True\n",
       "ind_actividad_cliente     True\n",
       "renta                     True\n",
       "segmento                  True\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1           True\n",
       "ind_nom_pens_ult1         True\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace other missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove rows with any nan value left\n",
    "df = df.dropna(subset=['renta', 'segmento', 'canal_entrada', 'ind_empleado', \n",
    "                       'pais_residencia', 'indresi', 'indresi', 'sexo'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado             False\n",
       "pais_residencia          False\n",
       "sexo                     False\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                False\n",
       "antiguedad               False\n",
       "indrel                   False\n",
       "indrel_1mes              False\n",
       "tiprel_1mes              False\n",
       "indresi                  False\n",
       "indext                   False\n",
       "canal_entrada            False\n",
       "indfall                  False\n",
       "nomprov                  False\n",
       "ind_actividad_cliente    False\n",
       "renta                    False\n",
       "segmento                 False\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1          False\n",
       "ind_nom_pens_ult1        False\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values are gone\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               81712\n",
       "ncodpers                 81712\n",
       "ind_empleado             81712\n",
       "pais_residencia          81712\n",
       "sexo                     81712\n",
       "age                      81712\n",
       "fecha_alta               81712\n",
       "ind_nuevo                81712\n",
       "antiguedad               81712\n",
       "indrel                   81712\n",
       "indrel_1mes              81712\n",
       "tiprel_1mes              81712\n",
       "indresi                  81712\n",
       "indext                   81712\n",
       "canal_entrada            81712\n",
       "indfall                  81712\n",
       "nomprov                  81712\n",
       "ind_actividad_cliente    81712\n",
       "renta                    81712\n",
       "segmento                 81712\n",
       "ind_ahor_fin_ult1        81712\n",
       "ind_aval_fin_ult1        81712\n",
       "ind_cco_fin_ult1         81712\n",
       "ind_cder_fin_ult1        81712\n",
       "ind_cno_fin_ult1         81712\n",
       "ind_ctju_fin_ult1        81712\n",
       "ind_ctma_fin_ult1        81712\n",
       "ind_ctop_fin_ult1        81712\n",
       "ind_ctpp_fin_ult1        81712\n",
       "ind_deco_fin_ult1        81712\n",
       "ind_deme_fin_ult1        81712\n",
       "ind_dela_fin_ult1        81712\n",
       "ind_ecue_fin_ult1        81712\n",
       "ind_fond_fin_ult1        81712\n",
       "ind_hip_fin_ult1         81712\n",
       "ind_plan_fin_ult1        81712\n",
       "ind_pres_fin_ult1        81712\n",
       "ind_reca_fin_ult1        81712\n",
       "ind_tjcr_fin_ult1        81712\n",
       "ind_valo_fin_ult1        81712\n",
       "ind_viv_fin_ult1         81712\n",
       "ind_nomina_ult1          81712\n",
       "ind_nom_pens_ult1        81712\n",
       "ind_recibo_ult1          81712\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Machine learning pipeline Implementation\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any previous spark session and check df file type\n",
    "spark.stop()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark SQL dataframe \n",
    "## IMPORTANT!! - this cell usually takes time due to data volume!!!\n",
    "## IMPORTANT!! - Only run this cell once! (to run it again, you need to restart the kernel)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlCtx = SQLContext(sc) #print(sc)\n",
    "df_spark = sqlCtx.createDataFrame(df)\n",
    "type(df_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define datatypes in dataframe\n",
    "\n",
    "df_spark = df_spark.select(df_spark.fecha_dato.cast(\"date\"),\n",
    "                                   df_spark.ncodpers.cast(\"float\"),\n",
    "                                   df_spark.ind_empleado.cast(\"string\"),\n",
    "                                   df_spark.pais_residencia.cast(\"string\"),\n",
    "                                   df_spark.sexo.cast(\"string\"),\n",
    "                                   df_spark.age.cast(\"float\"),\n",
    "                                   df_spark.fecha_alta.cast(\"date\"),\n",
    "                                   df_spark.ind_nuevo.cast(\"float\"),\n",
    "                                   df_spark.antiguedad.cast(\"float\"),\n",
    "                                   df_spark.indrel.cast(\"float\"),\n",
    "                                   df_spark.indrel_1mes.cast(\"float\"),\n",
    "                                   df_spark.tiprel_1mes.cast(\"string\"),\n",
    "                                   df_spark.indresi.cast(\"string\"),\n",
    "                                   df_spark.indext.cast(\"string\"),\n",
    "                                   df_spark.canal_entrada.cast(\"string\"),\n",
    "                                   df_spark.indfall.cast(\"string\"),\n",
    "                                   df_spark.nomprov.cast(\"string\"),\n",
    "                                   df_spark.ind_actividad_cliente.cast(\"float\"),\n",
    "                                   df_spark.renta.cast(\"float\"),\n",
    "                                   df_spark.segmento.cast(\"string\"),\n",
    "                                   df_spark.ind_ahor_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_aval_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cder_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cno_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctju_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctma_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctop_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctpp_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deme_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_dela_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ecue_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_fond_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_hip_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_plan_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_pres_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_reca_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_tjcr_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_valo_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_viv_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nomina_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nom_pens_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_recibo_ult1.cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha_dato: date (nullable = true)\n",
      " |-- ncodpers: float (nullable = true)\n",
      " |-- ind_empleado: string (nullable = true)\n",
      " |-- pais_residencia: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- fecha_alta: date (nullable = true)\n",
      " |-- ind_nuevo: float (nullable = true)\n",
      " |-- antiguedad: float (nullable = true)\n",
      " |-- indrel: float (nullable = true)\n",
      " |-- indrel_1mes: float (nullable = true)\n",
      " |-- tiprel_1mes: string (nullable = true)\n",
      " |-- indresi: string (nullable = true)\n",
      " |-- indext: string (nullable = true)\n",
      " |-- canal_entrada: string (nullable = true)\n",
      " |-- indfall: string (nullable = true)\n",
      " |-- nomprov: string (nullable = true)\n",
      " |-- ind_actividad_cliente: float (nullable = true)\n",
      " |-- renta: float (nullable = true)\n",
      " |-- segmento: string (nullable = true)\n",
      " |-- ind_ahor_fin_ult1: float (nullable = true)\n",
      " |-- ind_aval_fin_ult1: float (nullable = true)\n",
      " |-- ind_cco_fin_ult1: float (nullable = true)\n",
      " |-- ind_cder_fin_ult1: float (nullable = true)\n",
      " |-- ind_cno_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctju_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctma_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctop_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctpp_fin_ult1: float (nullable = true)\n",
      " |-- ind_deco_fin_ult1: float (nullable = true)\n",
      " |-- ind_deme_fin_ult1: float (nullable = true)\n",
      " |-- ind_dela_fin_ult1: float (nullable = true)\n",
      " |-- ind_ecue_fin_ult1: float (nullable = true)\n",
      " |-- ind_fond_fin_ult1: float (nullable = true)\n",
      " |-- ind_hip_fin_ult1: float (nullable = true)\n",
      " |-- ind_plan_fin_ult1: float (nullable = true)\n",
      " |-- ind_pres_fin_ult1: float (nullable = true)\n",
      " |-- ind_reca_fin_ult1: float (nullable = true)\n",
      " |-- ind_tjcr_fin_ult1: float (nullable = true)\n",
      " |-- ind_valo_fin_ult1: float (nullable = true)\n",
      " |-- ind_viv_fin_ult1: float (nullable = true)\n",
      " |-- ind_nomina_ult1: float (nullable = true)\n",
      " |-- ind_nom_pens_ult1: float (nullable = true)\n",
      " |-- ind_recibo_ult1: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# IMPORTANT - Define target label (for prediction) from target features\n",
    "labels = \"ind_ctju_fin_ult1\"\n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "    \n",
    "# define variables; categorical, countinuous and target features\n",
    "\n",
    "numericCols = [\"age\",\"antiguedad\",\"renta\"]\n",
    "\n",
    "categoricalColumns = [\"ind_empleado\",\"pais_residencia\",\"sexo\",\"ind_nuevo\",\"indrel\", \n",
    "                      \"indrel_1mes\",\"tiprel_1mes\", \"indresi\", \"indext\", \"canal_entrada\",\"nomprov\", \n",
    "                      \"ind_actividad_cliente\",\"segmento\"]\n",
    "\n",
    "targetsColumns = [\"ind_ahor_fin_ult1\", \"ind_aval_fin_ult1\",\n",
    "                        \"ind_cco_fin_ult1\", \"ind_cder_fin_ult1\", \"ind_cno_fin_ult1\",\n",
    "                        \"ind_ctma_fin_ult1\", \"ind_ctop_fin_ult1\",\n",
    "                        \"ind_ctpp_fin_ult1\", \"ind_deco_fin_ult1\", \"ind_deme_fin_ult1\", \n",
    "                        \"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\", \"ind_fond_fin_ult1\",\n",
    "                        \"ind_hip_fin_ult1\", \"ind_plan_fin_ult1\", \"ind_pres_fin_ult1\",\n",
    "                        \"ind_reca_fin_ult1\", \"ind_tjcr_fin_ult1\", \"ind_valo_fin_ult1\", \n",
    "                        \"ind_viv_fin_ult1\", \"ind_nomina_ult1\", \"ind_nom_pens_ult1\",\"ind_recibo_ult1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\") # Category Indexing with StringIndexer\n",
    "    stages += [stringIndexer]  # Add stages to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define categorical index columns \n",
    "categoricalColumnsIDX = [\"ind_empleadoIndex\",\"pais_residenciaIndex\",\"sexoIndex\",\n",
    "                         \"ind_nuevoIndex\",\"indrelIndex\",\"indrel_1mesIndex\",\n",
    "                         \"tiprel_1mesIndex\",\"indresiIndex\",\"indextIndex\", \n",
    "                         \"canal_entradaIndex\",\"nomprovIndex\",\"ind_actividad_clienteIndex\",\"segmentoIndex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = labels,\n",
    "                                outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "assemblerInputs = categoricalColumnsIDX + numericCols + targetsColumns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs,\n",
    "                            outputCol = \"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prePipeline = Pipeline(stages = stages)\n",
    "pipelineModel = prePipeline.fit(df_spark)\n",
    "\n",
    "dataset = pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol = \"label\", \n",
    "                            featuresCol = \"features\", \n",
    "                            numTrees = 100,                 #  Number of trees in the random forest\n",
    "                            impurity = 'entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy = \"auto\",\n",
    "                            predictionCol = \"prediction\",\n",
    "                            maxDepth = 5, \n",
    "                            maxBins = 50, \n",
    "                            minInstancesPerNode = 2) \n",
    "                            #minInfoGain=0.0, \n",
    "                            #subsamplingRate=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section C) Evaluation of Performance and testing (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (don’t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set\n",
    "[trainData, testData] = dataset.randomSplit([0.8, 0.2], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "# evaluation of model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", \n",
    "                                              predictionCol = \"prediction\", \n",
    "                                              metricName = \"accuracy\")\n",
    "# random forest parameters\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "# Method: pyspark.ml.tuning.TrainValidationSplit\n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting cross-validation')\n",
    "cvModel = crossval.fit(trainData)  # This takes time!\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.999067\n",
      "Training Error = 0.00093345\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(trainData)\n",
    "train_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Training Accuracy = %g\" % (train_accuracy))\n",
    "print(\"Training Error = %g\" % (1.0 - train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.998105\n",
      "Test Error = 0.00189452\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % (test_accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section D) Implement a parameter grid - Model fine-tuning (35%) \n",
    "\n",
    "### Note: This section takes time to compute!\n",
    "\n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Evaluate model performance using a subset of preprocessing variables\n",
    "#### No numeric predictors used, relaunch pipeline with this new preprocessing structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New preprocessing stage, without numeric predictors\n",
    "new_stages = []\n",
    "\n",
    "# remove preprocessing numeric predictors by including an empty vector\n",
    "New_numericCols = [] # empty numeric predictors\n",
    "\n",
    "# Add Newstages to the pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\")\n",
    "    new_stages += [stringIndexer]  # Add stages to the pipeline\n",
    "\n",
    "new_stages += [label_stringIdx]\n",
    "\n",
    "# empty vector is inserted here\n",
    "new_assemblerInputs = categoricalColumnsIDX + New_numericCols + targetsColumns\n",
    "new_assembler = VectorAssembler(inputCols = new_assemblerInputs, outputCol = \"features\")\n",
    "\n",
    "new_stages += [new_assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating new pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "new_prePipeline = Pipeline(stages = new_stages)\n",
    "new_pipelineModel = new_prePipeline.fit(df_spark)\n",
    "\n",
    "new_dataset = new_pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.999067\n",
      "Training Error = 0.00093345\n",
      "Test Accuracy = 0.998105\n",
      "Test Error = 0.00189452\n"
     ]
    }
   ],
   "source": [
    "[new_trainData, new_testData] = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "new_cvModel = crossval.fit(new_trainData)  # This takes time!\n",
    "\n",
    "# Results:\n",
    "\n",
    "new_predictions = cvModel.transform(new_trainData)\n",
    "new_train_accuracy = evaluator.evaluate(new_predictions)\n",
    "print(\"New Training Accuracy = %g\" % (new_train_accuracy))\n",
    "print(\"New Training Error = %g\" % (1.0 - new_train_accuracy))\n",
    "\n",
    "new_test_predictions = cvModel.transform(new_testData)\n",
    "new_test_accuracy = evaluator.evaluate(new_test_predictions)\n",
    "print(\"New Test Accuracy = %g\" % (new_test_accuracy))\n",
    "print(\"New Test Error = %g\" % (1.0 - new_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size evaluation\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n",
      "\n",
      "\n",
      "=== training set of size 100%, wait please\n",
      "Classification Error = 0.00093345\n"
     ]
    }
   ],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "%time\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n\\n=== training set of size 100%, wait please')\n",
    "cvModel = crossval.fit(trainData)\n",
    "predictions = cvModel.transform(trainData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.5 µs\n",
      "\n",
      "\n",
      "=== training set of size reduced to 50.0%, wait please\n",
      "Classification Error = 0.00301217\n",
      "\n",
      "\n",
      "=== training set of size reduced to 10.0%, wait please\n",
      "Classification Error = 0.00729853\n",
      "\n",
      "\n",
      "=== training set of size reduced to 5.0%, wait please\n",
      "Classification Error = 0.00140168\n",
      "\n",
      "\n",
      "=== training set of size reduced to 1.0%, wait please\n",
      "Classification Error = 0.00433839\n",
      "\n",
      "\n",
      "=== training set of size reduced to 0.1%, wait please\n",
      "Classification Error = 0.00980392\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "i = 0\n",
    "for split in data:\n",
    "    print('\\n\\n=== training set of size reduced to {}%, wait please'.format(sizes[i]*100))\n",
    "    cvModel = crossval.fit(split)\n",
    "    predictions = cvModel.transform(split)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - accuracy))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 µs\n",
      "starting Hyperparameter Grid Search with cross-validation\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "%time\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,50,100]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1,3,5]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,8]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "cvModel = crossval.fit(trainData)\n",
    "print('Grid Search with cross-validation has finished')\n",
    "\n",
    "# pick best model\n",
    "rfModel = cvModel.bestModel\n",
    "print (rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = rfModel.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Random Forest perform very well over this data due to its abstract nature. The average performance is practically perfect when executing the pipeline using Test Data (XXX Accuracy). The level of abstraction and flexibility is so high, that even when some predictors are removed the accuracy remains its levels, as seen in section 4.1\n",
    "\n",
    "Furthermore, the different sizes of the dataset studied show that bigger sizes provide better results, because the number of cases (rows) seen by the algorithm increases the flexibility and the performance of the algorithm. Besides, bigger sizes avoid randomness. As seen in section 4.2, the accuracy varies widely depending on the luck. If the sample has good example cases the acuracy would be high, if you are unlucky and the cases are not representative the accuracy will be low. This is why the size in the sample is so important.\n",
    "\n",
    "Moreover, as backed in theory a lot of small trees (low depth) provided good results with less computational cost. Because depth trees usually provides overfitting. This is studied in section 4.3\n",
    "\n",
    "To sum up, Random Forest is a very good approach to this problem because of the nature of the data, large and complex. The good results obtained is the main proof."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
